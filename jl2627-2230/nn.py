# -*- coding: utf-8 -*-
"""nn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nerazlyPC3BNY7wj8zma3WWJXduU0h8n
"""

import csv 
import numpy as np
import time 
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.utils import check_random_state
from sklearn.neural_network import MLPClassifier
import matplotlib.pyplot as plt

# download data 
!wget https://pjreddie.com/media/files/mnist_train.csv
!wget https://pjreddie.com/media/files/mnist_test.csv

# test_set is the list of the 10000 test digits, each represented as a row vector 
test_set = np.genfromtxt('mnist_test.csv',delimiter=',')

test_labels = np.array(list(map(lambda x: x[0], test_set)))
test_digits = np.array(list(map(lambda x: x[1:], test_set)))

def error_rate(pred):
  '''Takes list of predictions and returns ratio of
     wrong predictions to total.'''
  counter = 0
  assert(len(pred) == len(test_set))
  for i, l in enumerate(pred):
    if l != test_set[i][0]:
      counter += 1
  return counter / len(test_set)

train_set = np.genfromtxt('mnist_train.csv',delimiter=',')

training_labels = np.array(list(map(lambda x: x[0], train_set)))
training_digits = np.array(list(map(lambda x: x[1:], train_set)))

# n = 5 time to train
start = time.perf_counter()
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(training_digits,training_labels)
end = time.perf_counter()
print(end - start)

# n = 5 time to predict
start = time.perf_counter()
pred = knn.predict(test_digits)
end = time.perf_counter()
print(f'error rate: {error_rate(pred)}\ntime: {end - start}')

# n = 5, 500 training examples
training_digits_500 = training_digits[0:500]
training_labels_500 = training_labels[0:500]

start = time.perf_counter()
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(training_digits_500,training_labels_500)
end = time.perf_counter()
print(end - start)

start = time.perf_counter()
pred = knn.predict(test_digits)
end = time.perf_counter()
print(f'error rate: {error_rate(pred)}\ntime: {end - start}')

# n = 50 time to train
start = time.perf_counter()
knn50 = KNeighborsClassifier(n_neighbors=50)
knn50.fit(training_digits,training_labels)
end = time.perf_counter()
print(end - start)

# n = 50 time to predict
start = time.perf_counter()
pred = knn50.predict(test_digits)
end = time.perf_counter()
print(f'error rate: {error_rate(pred)}\ntime: {end - start}')

# n = 50, 500 training examples
start = time.perf_counter()
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(training_digits_500,training_labels_500)
end = time.perf_counter()
print(end - start)

start = time.perf_counter()
pred = knn.predict(test_digits)
end = time.perf_counter()
print(f'error rate: {error_rate(pred)}\ntime: {end - start}')

# for cut and paste
def timer():
  start = time.perf_counter()
  end = time.perf_counter()
  print(end - start)
  print(f'error rate: {error_rate(pred)}\ntime: {end - start}')

# softmax training
start = time.perf_counter()
scaler = StandardScaler()
training_digits = scaler.fit_transform(training_digits)
test_digits = scaler.transform(test_digits)
sft = LogisticRegression(C=1e5,
                         multi_class='multinomial',
                         penalty='l2', solver='sag', tol=0.1)
sft.fit(training_digits, training_labels)
end = time.perf_counter()
print(end - start)

# softmax prediction
def time_pred(clf):
  lst1 = []
  lst2 = []
  for i in range(0,100):
    start = time.perf_counter()
    pred = clf.predict(test_digits)
    end = time.perf_counter()
    lst1.append(end-start)
    lst2.append(error_rate(pred))

  print(f'error rate avg: {sum(lst2)/len(lst2)}\ntime avg: {sum(lst1)/len(lst1)}')

time_pred(sft)

# visualizion of weights w_i
def viz(coef, s):
    plt.figure(figsize=(10, 5))
    scale = np.abs(coef).max()
    for i in range(10):
        l2_plot = plt.subplot(2, 5, i + 1)
        l2_plot.imshow(coef[i].reshape(28, 28), interpolation='nearest',
                      cmap=plt.cm.Greys, vmin=-scale, vmax=scale)
        l2_plot.set_xticks(())
        l2_plot.set_yticks(())
        l2_plot.set_xlabel('Class %i' % i)
    plt.suptitle(s)
    plt.show()

viz(sft.coef_.copy(), 'image form of weights vector $w_i$ for digit class $i$')

# 10 by 10
start = time.perf_counter()
scaler = StandardScaler()
scaler.fit(training_digits)
training_digits = scaler.transform(training_digits)
test_digits = scaler.transform(test_digits)
clf = MLPClassifier(solver='adam', alpha=1e-5,
                     hidden_layer_sizes=(10, 10), random_state=1)
clf.fit(training_digits, training_labels)
end = time.perf_counter()
print(f'{end - start}')

time_pred(clf)

# 500 by 300
start = time.perf_counter()
scaler = StandardScaler()
scaler.fit(training_digits)
training_digits = scaler.transform(training_digits)
test_digits = scaler.transform(test_digits)
clf = MLPClassifier(solver='adam', alpha=1e-5,
                     hidden_layer_sizes=(500, 300), random_state=1)
clf.fit(training_digits, training_labels)
end = time.perf_counter()
print(f'{end - start}')
time_pred(clf)